# 网络编程实战

> 来源：知乎--“想飞的猫”

## 一：基础篇

**一，Unix / Linux系统，TCP / IP 协议**

历史溯源，略

**二，网络编程模型的基本概念**

**1. 客户端-服务端的网络编程模型：**三段论，客户端按照双方约定的格式，向服务端发送请求；服务端按约定的格式解释请求，并开始处理；服务端将处理结果按照同样的格式编码，并响应给客户端。无论是客户端还是服务端，它的运行单位都是进程，而不是机器，因此，对于一个终端，同一时刻可以建立多个不同的服务器连接。同一个服务器，也可以运行多个服务。

**2. IP 和端口：**IP 地址全网唯一，标识某一台机器，端口是一个 16 位的整数，最大为 65535。服务端监听在一个众所周知的端口上，客户端端口是发起连接请求时，系统内核临时分配的。四元组唯一确定一个连接，也叫套接字对。（客户端 IP : 端口，服务端 IP : 端口）

**3. 保留网段：**在 IP V4 的地址空间里，专门划出 3 个保留网段，仅仅作为机构内部使用。

![img](.\Image\网络编程实战-1.jpg)

**4. 子网掩码**

- **网络：**一组 IP 中的共同部分。比如，192.168.1.1 - 192.168.1.255 的区间里，192.168.1.0 代表所在的网络。
- **主机：**一组 IP 中的不同部分，比如，上例中的 1 - 255，表示在这个网络中，有 255 个可用的 IP 地址。
- **子网掩码：**决定一个 IP 所在子网的网络地址，它能用任意位数表示，每一位都是 1。通过与网络掩码的“位与操作”，可以切出一个 IP 所在的子网，以及它对应的主机号。子网掩码有不同的表示方法。

**5，全球域名系统（DNS）：**记录并查找网站地址和其对应 IP 地址的映射关系。

**6，字节流和数据报：**TCP / UDP 协议。

**三，套接字和地址**

**1，服务端：**首先初始化监听 socket，将其绑定 bind 在一个众所周知的地址和端口上，接着开启监听 listen，最后阻塞在 accept 上等待客户端连接请求到来。

**2，客户端：**首先初始化连接 socket，接着向服务端的地址和端口发起连接 connect 请求，执行 TCP 三次握手。

**3，数据传输：**连接建立后，客户端向内核发起 write 的系统调用执行写操作，发送请求，数据从应有程序被拷贝到内核协议栈，协议栈将字节流通过网络设备传输到服务端的内核协议栈，服务端通过 read 系统调用，将协议栈中客户端传输的数据，拷贝到应用程序中，进行解析，并执行业务处理后，以同样的方式写给客户端进行响应。因此，**一旦连接建立，数据传输是双向的，这点需要被牢记！**

**4，连接关闭：**当客户端需要和服务端断开连接时，调用 close 函数。**此时发生的操作是**，系统内核向该连接链路上的服务端发送一个 FIN 包，服务端收到后执行被动关闭，此时，客户端在收到服务端反馈前，认为连接是正常的，此时整个链路处于半关闭的状态。当服务端执行被动关闭时，也会调用 close 函数，此时整个链路才进入全关闭状态，双方都会感知到连接已关闭。

**5，套接字地址格式：**通用地址结构（16字节），IP V4 地址结构（16字节），IP V6 地址结构（28字节），本地地址结构（最多 110 字节：本地文件无需端口，路径不同地址可变）。

使用通用地址结构，是对其他地址结构的抽象，有一个可以统一操作的地址结构就可以设计统一的网络编程接口。开发人员在具体实现中，再根据通用地址内的协议族，强制转换为指定的地址类型即可。

**6，HTTP，Web Socket 的区别和联系：**HTTP是应用层协议，基于TCP Socket实现，通常是短连接，客户端只能不断轮询从服务端获得消息。WebSocket是对HTTP的增强，利用TCP双向特性，增强服务端到客户端的传输能力，服务端可以直接推送消息到客户端。

**四，连接与收发**

**1，服务端监听过程：**

**1)，socket 创建套接字：**domain：协议类型（IPV4，IPV6，LOCAL）；type（字节流，数据报）

**2)，bind 绑定地址端口：**fd：监听套接字；addr：服务端通用地址指针（前两个字节，判断协议类型）；len：addr 的地址长度（处理可变长度）；

绑定时，地址和端口有多种处理方式：

**如果绑定指定地址**，则系统内核只会接收目标地址是该指定地址的 IP 包，供服务端处理。但如果一个服务器上有多个网卡，这个方式显然不合理，因为有多个地址都对应这台服务器，但服务端只接收一个地址上数据包。所以，可以考虑在服务端使用通配地址，即 IPV4：INADDR_ANY；IPV6：IN6ADDR_ANY。

**如果绑定端口0，**则服务端会把端口选择交给系统内核，内核会根据算法选择特定端口，这显然也不合理。因为，服务端的端口通常需要绑定在众所周知的非保留端口上。

**3)，listen 开启监听：**socketfd：监听套接字；backlog：未完成连接队列的大小，表示 socketfd 可读，可以 accpet 建立连接，但还没来得及 accept 的连接数。Linux 不允许改变对该参数。

默认情况下，创建一个普通的 socket，被认为是需要主动发起请求的客户端套接字。通过 listen 函数，可以将“主动”转换为“被动”，系统内核会做好被动接收请求的准备，初始化对应的数据结构，比如，完成连接队列。

**4)，accept 接收连接：**listen sockfd：监听套接字；cliaddr：客户端地址；addrlen：客户端地址长度。返回值：连接套接字，服务端可以通过该返回的套接字，与特定客户端通信。

**2，客户端连接过程：**

- **socket 创建套接字：同上**
- **connect 发起连接：**sockfd：连接套接字；servaddr：服务端地址；addrlen：地址长度。
- **开始三次握手：**成功或出错后，才返回。**出错的情况：**
  - 1）客户端 SYN 包无响应，客户端返回 TIME OUT 超时错误，通常是服务端IP填错；
  - 2）客户端收到 RST（复位）响应，客户端返回 CONNECTION REFUSED 错误，通常是端口写错，因为 RST 表示目的端口的 SYN 包到达，但该端口并未开启任何监听，TCP 发现它接收到一个根本不存在的连接上的数据包，取消该连接；
  - 3）客户端 SYN 包目的地不可达，通常是服务端与客户端的路由不通。

**3，客户端发送数据：**

- **常使用的函数：**write，send 和 sendmsg，场景略有不同，都是以 socketfd 为目标写，但 send 和 sendmsg 分别还可用户发送带外紧急数据和多重缓冲区传输。和写普通文件不同的时，write 在普通文件写中，真正写入的大小通常与传入的 size 相同，而对套接字而言，真正写入的大小有可能比传入的 size 小，原因和系统内核建立的发送缓冲区有关。
- **发送缓冲区：**SO_SNDBUF，write 函数实际上是将数据从应用程序拷贝到系统内核的发送缓冲中，由内核决定何时发送。因此，对于阻塞套接字来说，如果剩余发送缓冲区够大，写入的数据量就是请求的数据量，相反，如果剩余发送缓冲区足够小，wirte 会阻塞挂起，直到内核发送完数据，腾出足够的空间后，write 写入全部数据后返回。而对非阻塞套接字，能发送多少就返回多少，返回值是写入发送缓冲区的长度。
- **无限增大系统内核缓冲区**，可以减少 write 和 read 系统调用，降低状态的切换次数，但对吞吐量意义不大，内核发送时，会受到 IP 包 MTU 的最大限制。

**4，服务端接受数据：**

- **常使用的函数：**read，以 socketfd 为目标读，要求最多读取的字节数，返回实际读取的字节数。阻塞套接字下的**特殊返回值**：
  - 返回0，表示对端发送 close 连接，接受到它发送的 FIN 包，已无数据可读，要处理断连的情况；
  - 返回-1，表示读取出错。对非阻塞套接字来说，返回 -1 时，还需要判断 errno 的值，后面细讲。

- **接收缓冲区：**同样，read 是指程序将数据从系统内核拷贝至应用程序。阻塞套接字会挂起，直到有数据返回。而非阻塞套接字会直接返回。

**五，别忘了UDP**

**1，**最大**区别：**如果说 TCP 是日常普通打电话，面向连接，在 IP 报文的基础上，增加了重传，确认，有序和拥塞控制等能力，有一个确定的上下文；UDP 就是邮筒投递明信片，不可靠通信，有效，有序，拥塞啥都没有。

**2，使用场景：**UDP 比较简单，常见的 DNS 服务，SNMP 服务，聊天室，游戏等都使用 UDP，对丢包不敏感，丢一些不会影响实际体验。

**3，UDP编程：**通过 recvfrom 和 sendto 直接收发数据，若服务端不存在，客户端会阻塞在 recvfrom，而不是报错返回，应用层需要增加超时处理。若服务端开启->关闭->开启，不影响收发数据的正常运行，无需重新建立连接，表示 UDP 无上下文。

**4，最大报文长度**：TCP 包头中无“包长度字段”，完全依靠 IP 层 MTU 去处理分帧，协议本身也会进行拥塞控制和流量控制，所以它被称为“流协议”。UDP 包头中的包长度占 2 个字节，最大发送 65535 字节的数据，扣除 UDP 头的 8 个字节和 IP 头的 20 个字节，UDP 包最多发送 65507 字节的数据。IP 包的最大长度受到数据链路层 1500 个字节（不含链路层包头）的限制，因此，对 UDP 包来说，它在 IP 层的 MTU 是 1500 - 20(IP头) - 8(UDP头) = 1472字节；TCP 的 MTU 是是 1500 - 20(IP头) - 20(TCP头) = 1460 字节。

**六，还有本地套接字**

本地套接字是 IPC，是一种本地进程间的通信方式，除此之外，还有管道，共享内存，共享消息队列等。

**1，最大区别：**使用 TCP/IP 的 127.0.0.1 完成在一台机器上客户端和服务端的进程通信，也要通过网卡回环走网络协议栈，而本地套接字本身就是一种单主机跨进程的调用手段，不会通过网络，效率更高；因此，本地套接字地址只需要输入协议号 AF_LOCAL 和以绝对路径表示的 socket 文件名即可，与端口无关。

**2，错误场景：**当只启动客户端时，文件套接字上没有被监听，客户端直接报错，提示文件不存在。当服务端监听在没有权限的文件上，服务端直接报错，提示无权限监听。

**3，使用方式：**本地套接字和其他套接字的编程接口相同，同时支持数据流和数据报2种协议；因为不走网络协议栈，因此效率上大大高于前两者，可以作为同一机器不同进程间的 IPC 通信工具使用。

**七，学习网络编程的6大工具**

**1，ping，网络连通性测试**，基于 ICMP 协议，是一种基于 IP 的控制协议，与 TCP 或 UDP 无关；源主机组装 ICMP 协议发送报文，IP 报文通过 ARP 协议，源地址和目的地址都被翻译成 MAC 地址，经过数据链路层后发送出去，目的主机再以相同的方式应答。

**2，tracerout**，网络路由转发测试，也是基于 ICMP 协议，使用 TTL 计数的方式，检查报文从源地址到目标地址所经过的路由器。

**3，ifconfig，**显示当前系统的网卡列表。详细显示了网卡支持的协议，IP 地址，MAC 地址，广播地址，子网掩码，Metric 等等信息。广播地址是子网中一个特殊的被保留的地址，向该地址发消息，子网络中的所有主机都能收到。通常是使用 UDP 实现的。当本机向目的地址发起连接时，数据可以从多个网卡发送出去，具体的网卡优先级选择由 Metric 来决定，数值越小，优先级越高。

**4，netstat：**帮助了解本机上所有的网络连接状态，并明晰连接详情。在本机端口不足时，用来检查无效的 TIME_WAIT 连接最有效。

**5，lsof：**帮助找出指定 IP 地址和端口上打开的套接字进程。在本机地址被占用时，排查正在使用该端口的进程最有用。

**6，tcpdump：**抓包。开启抓包时，tcpdump 会自动创建一个 AF_PACKET 协议的网络套接口并向内核注册。当网卡接收到一个网络报文后，会遍历所有已注册的网络协议，并调用绑定在其上的回调函数，回调函数内可以自行处理，比如，完整复制一份报文等，再转交给 tcpdump 程序进行条件的选择和过滤。

**八，问题：**一段数据流从服务端进程至远端的客户端进程，数据包会经过几次拷贝？

![img](.\Image\网络编程实战-2.jpg)



**九，参考文章：**

1，极客时间：网络编程实战 —— 盛延敏：（基础篇1-9讲）

## 二：提高篇_上

**一，TIME_WAIT**

**1，来源：**TCP三次握手建立连接，四次挥手断开连接，在四次挥手的过程中，发起断开请求的一方会有一段时间进入TIME_WAIT，具体时机，详见下图：

![img](.\Image\网络编程实战-3.jpg)

**2，时长：**客户端在 TIME_WAIT 的停留时间是固定的，被指定为 2MSL，MSL 是规范定义的网络报文的最大生存时间，即，如果网络中的一个报文段在 MSL 时间内，未被接收，就会被直接丢弃。Linux 在实现中，指定 MSL = 30秒，因此，TIME_WAIT 的时间就规定为60秒。

**3，危害：**客户端是发起连接的一方，通常也是主动断开连接的一方，如果客户端对 TIME_WAIT 处理不当，会导致其所在的服务器在高并发的情况下，TIME_WAIT 状态的连接过多，将本地端口耗尽，在一段时间内，无法发起新的连接，直到 TIME_WAIT 被系统自动回收关闭。

**4，作用：**在TCP的容错设计，协议假设报文会出错，导致重传的前提下：

**4.1，保证客户端正常关闭：**如果服务端没有收到客户端第一次发出的ACK(n+1)，会在一来一回的时间范围内（2MSL）再次发出FIN(n)的请求，客户端会重传ACK(n+1)，TIME_WAIT重新计时。因此，如果在发出ACK(n+1)后的2MSL内没有收到服务端重传的FIN(n)，表示服务端确认关闭成功，自己可以正式关闭。

**4.2，保证服务端正常关闭：**如果客户端不经过TIME_WAIT阶段，而在ACK(n+1)后直接关闭连接，如果服务端没有收到客户端的ACK，重传FIN(n)，因为客户端端已经关闭，会触发服务端接收到RST分节，导致服务端认为客户端发生错误，触发自己的异常逻辑。

**4.3，避免连接“化身”**：正常情况下，协议栈会自动丢弃“迷走”在不存在的连接上的报文，造成迷走的原因是原连接中断，比如，路由器重启或链路故障灯原因。但如重新建立的另一个连接，系统新分配给客户端的端口正好和上个连接的端口相同，迷走的报文会被送达到协议栈，导致问题。为了避免这样的情况发生，客户端等待 2MSL，可以使得这条连接上的报文分段自然消失。

在 RFC1323 实现的扩展规范中，引入新的 2 个时间戳字段，分别表示 TCP 发送方的当前时间戳和从对端接收到的最新时间戳，每次新建连接时，时间戳会被重置。因此，连接化身的问题也得到了更妥善安全的处理。（net.ipv4.tcp_timestamps=1默认开启）

**5，优化：**既然TIME_WAIT如此重要又无法避免，可以有哪些方式优化呢？

**5.1，设置套接字 SO_LINGER 选项（不推荐）：**调整调用close或shutdown时的行为。SO_LINGER选项对应的结构体是linger，它包含l_onoff和l_linger2个变量，其中l_onoff表示选项是否开启：默认情况下，如果l_onoff设置为0，close或shutdown会立即返回，发送缓冲区如果仍有待发送数据，系统内核会尝试将他们发送出去，也会进入TIME_WAIT时间。但是，**如果将l_onoff设置为1**，则需要判断l_linger的具体数值：

**（1），如果l_linger设置为0**，主动关闭方会跳过TIME_WAIT，立即发送RST标志给对端并强行关闭连接。同时，排队的待发送数据将被清空。被动关闭方无法感知对端已直接断开，它将在read时返回“connect reset by peer”的异常。

**（2），如果l_linger非0，**主动关闭方会阻塞在close或shutdown上，直到发送缓冲区数据发送完成或l_linger超时，仍然可能跳过TIME_WAIT。

**5.2，设置net.ipv4.tcp_tw_reuse（推荐）：**如果协议安全可控（由连接发起方主动关闭，同时进入TIME_WAIT的时间已超过1秒），那么在打开tcp时间戳支持（默认）的前提下，可以复用TIME_WAIT的套接字（时间戳能区分新旧连接 + 避免过多新端口的开启）。

**注意区分：**tcp_tw_reuse设置和SO_REUSEADDR选项，前者的目的是解决TIME_WAIT，在协议安全可控的前提下，通常是客户端，避免TIME_WAIT产生过多端口占用的问题。后者的目的是解决端口复用，通常是服务端，可以使服务端始终监听在一个众所周知的端口上，不会因为重启或挂掉而在一段时间能不能使用该端口。

**二，优雅关闭**

因为TCP是双向的，在大多数情况下，TCP连接都是先关闭一个方向，此时，另一个方向的数据还是可以正常传输的。比如，客户端发起关闭连接，只是关闭了客户端到服务端的写数据，但服务端读取到客户端的最后数据后，仍然可以向客户端发送数据，此时TCP连接处于“半关闭”状态，直到服务端有条不紊的处理完成后，服务端关闭连接，发送FIN(n)，进入最后的LAST_ACK关闭状态，客户端回应ACK(n+1)并进入TIME_WAIT，2MSL后，TCP连接优雅关闭。

上述描述只是理论上实现优雅关闭的过程，具体到实现上，内核提供了close和shutdown的系统调用，那么具体调用哪个函数，它们分别有什么区别？

**1，close：**使用引用计数

**1.1，套接字引用计数-1：**因为套接字可能被其他进程共享，使用fork产生子进程时，套接字引用计数+1，每close一次，引用计数-1；当引用计数为0时，才回真正关闭连接。

**1.2，同时关闭TCP两个方向的数据流：**

**1），输入方向上，**设置套接字不可读，应用程序任何读操作都会返回异常；

**2），输出方向上，**系统内核会将发送缓冲区的数据发送至对端，并发送FIN的控制报文，接下来对它的任何写都会导致异常。如果对端没有检测到该FIN的信号，继续向该应用发送数据，会收到RST的报文，表示连接已重置，不能再发送数据。如果对端没处理该RST异常，再一次发送数据，则会触发SIGPIPE信号，默认进程退出。

**2，shutdown：**使用howto参数，shutdown可以有选择的关闭一个方向的连接。

**1.1，SHUT_RD(0)：**关闭读方向，对端仍然可以发送数据，但系统内核ACK后便悄悄丢弃，因此，对该套接字的读都直接返回0（EOF）。

**1.2，SHUT_WR(1)：**关闭写方向，不管套接字的引用计数是多少，直接关闭连接的写方向，发送缓冲区的数据首先被发送出去，并紧接FIN的控制报文。后期对该套接字的写操作，会报错。

**1.3，SHUT_RDWR(2)：**SHUT_RD和SHUT_RD个操作一次，关闭读写2个方向。虽然看上去和close的效果一样，但实际上存在2个重要区别：close关闭连接会释放所有资源，而shutdown不会立即释放，直到双向连接都完成关闭；close有引用计数的概念，并不一定会使套接字不可用，因此也并不一定会发出FIN的结束报文，shutdown则直接使套接字不可用，同时总是会发出FIN结束报文，通知对端。

**三，检测连接状态**

在网络编程实战中，保持对连接有效性的检验，将过时的连接剔除，是必须要注意的点。但是在没有任何数据读写的“静默”连接上，客户端突然崩溃，服务端是无法发现的。因此，需要一种保活策略，在连接上告诉对端自己是否活跃。通常由2种做法：

**1，TCP Keep-Alive选项**

在**指定时间内(2h)**，如果连接上没有任何活动，TCP保活机制就开始作用，**每隔一段时间(75s)**就默认发送一个探测报文，如果**连续几个探测(9次)**报文都无响应。则认为TCP连接死亡，系统内核将错误信息提示给上层应用（加粗的3个变量是该机制需要设置的参数）。

这样，不管是对端正常工作（正常响应，时间重置），崩溃后重启（连接无效，回应RST）或崩溃断电（连续多次后，判断连接死亡），应用程序都能根据保活报文的应答，及时感知对端的情况，并作出响应处理。该机制默认关闭，单方向和双方向均可开启。

**2，应用层探活**

默认情况下，TCP Keep-Alive需要至少2h+75s*9的时间内，没有数据收发后，才会发送探活报文。在实际应用中，这是不可接受的。因此，必须在应用层，使用相同的逻辑，设置一套频率更高的探活机制。



**实战：务必手动实现一个简单的客户端-服务端探活机制，开启网络编程的实战生涯！**

**实战：务必手动实现一个简单的客户端-服务端探活机制，开启网络编程的实战生涯！**

**实战：务必手动实现一个简单的客户端-服务端探活机制，开启网络编程的实战生涯！**



**四，参考文章：**

1，极客时间：网络编程实战 —— 盛延敏：（提高篇10-12讲）

## 二：提高篇_下

**一，小数据包**

**1，流量控制与“生产者-消费者模型”**

调用send/write后，应用数据并没有被真正发送，只是被拷贝到了系统内核给这一个socket分配的发送缓冲区中。应用无法感知数据真正被发送的时机，它是由内核的TCP协议栈负责的。

发送窗口和接收窗口，也是针对发送缓冲区和接收缓冲区而言，他们作为TCP连接的双方，一个作为生产者，一个作为消费者，达到协同一致的生产-消费速率，而产生的算法模型。

即，发送者不能无限制的发送数据，需要考虑接收端中接收缓冲区的处理能力，而该处理能力是接收端通过发送窗口告诉服务端的。否则，大量接收端处理不过来的数据，会被丢弃导致重传，更多的数据报最终导致网络崩溃。

**2，拥塞控制与数据传输**

**流量控制只是考虑在单个连接上的数据传输**，但TCP数据包还需要经过网卡，交换机，路由器等一系列网络设备，而这些设备本身的能力也是有限的。所以，TCP协议必须考虑在有限的带宽上，如何**兼顾多个连接上的效率和公平性控制。对应的就是拥塞控制算法**。

TCP协议中的拥塞控制是通过拥塞窗口来完成的，而拥塞窗口本身是会随着网络状况实时调整的。**传统的拥塞控制**有2个阶段：慢启动，拥塞避免，拥塞发生。

2.1，**当一个TCP连接成功后，慢启动开始**，TCP从初始的拥塞窗口大小（cwnd=1，表示一个最大报文长度MSS大小的数据）开始，指数级（cwnd*2）将网络发送速率增加到一个阈值(ssthresh)，慢启动就结束了。

2.2，**慢启动后，拥塞避免开始**，TCP会不断探测网络状态，并不断调整拥塞窗口大小。比如，当cwnd中所有的报文段都被确认后，才将cwnd+1，然后慢慢增加到网络的最佳值，直到网络拥塞，比如，开始出现丢包，进入拥塞发生。

2.3，TCP有2种判断丢包的逻辑，**拥塞发生**，对应2种丢包后的重传机制：

**（1），超时重传：**在重传计时器的时间内，一个ack都没收到，网络太差，重传计时器超时重传；

**（2），快速重传：**连续收到3个及以上的相同ack，表示网络还可以，不用等超时，直接重传。

因此，拥塞发生后，对应也有2种不同的恢复方法：

**（1），拥塞严重（**发生超时重传）：ssthresh阈值减半，cwnd拥塞窗口置1，重新开始慢启动；

**（2），拥塞不严重**（发生快速重传）：首先cwnd拥塞窗口减半，ssthresh阈值重置为cwnd，启动快速恢复，对应有不同的TCP Reno算法。

因此，在任何一个时刻，TCP发送缓冲区内的数据是否真的发送出去，取决于min((单个TCP连接上，点对点流量控制中，发送端与接收端共同协商的发送窗口大小)，(多个TCP连接共享的带宽上，拥塞控制模型中，发送端独自根据网络状态来动态调整的拥塞窗口大小))。

**3，一些有趣的场景**

**3.1，糊涂窗口综合征：**接收端不能在刚刚读入非常少的字节数后，就向发送端发送更新窗口通知，而是应当在接收缓冲区大到一个合理范围后，才能发送。合理范围由RFC规范定义。

**3.2，Nagle算法：**在一些交互式场景下，比如，通过telnet或者ssh登录到一台远程服务器，发送端使用Linux命令行操作时，每次传输的数据可能非常小。根据其对实时性的要求，可以考虑开启或关闭Nagle算法。其中，Nagle算法提出，在任何一个时刻，未被确认的长度小于最大报文段长度MSS的TCP分组不能超过一个。这样，Nagle算法在接收端就可以把连续的几个小数据包存储起来，等待在途的小数据包被ACK确认后，再一次性发送出去。

**3.3，延迟ACK：**接收端如果对每一个TCP报文都使用不带任何数据分段，但又包含不可或缺元信息头的ACK报文进行确认，就会大量消耗带宽。因此，延时ACK表示，接收端在收到数据后并不会马上回复，而是在一段时间内累计需要发生的ACK报文，等待有数据需要发送给对端的时候，再将累计的ACK捎带一并发出。

虽然，Nagle算法和延迟ACK彼此阻碍，尤其是对时延敏感的交互式场景，可以通过设置Socket选项为TCP_NODELAY来关闭Nagle。但需要注意的是，现代计算机对两者的优化已经非常成熟，除非有十足的把握，否则不要轻易改变默认的Nagle算法。

**3.4，合并写操作：**使用writev或readv调用，配合iovec结构体数组，将多个请求一次发出，在应用层按需组合多个小数据包，避免Nagle算法引发的副作用。

**二，UDP“已连接”**

UDP是无连接协议，但是它可以调用connect函数。这里的connect并不会引起和目标服务器的网络交互，只是为了让应用程序接收“异步错误”的信息。

在传统UDP编程中，若服务端不开启，客户端会阻塞在recvfrom上，等待返回或超时。因此，通过UDP的socket进行connect操作，让该socket与服务端的地址和端口产生了联系，也给了系统内核必要的信息，使得内核收到的ICMP不可达信息可以反馈给socket。并且，该操作还能有一定程度的性能提升，减少了sendto发送信息时，频繁的地址初始化过程。

**三，地址已经被使用**

服务端程序首先断开连接并重启时，经常被出现地址已经被使用的情况，原因是以前提到的TIME_WAIT。

当客户端断开连接时，老的端口TIME_WAIT通常并不会有大的问题，因为客户端每次的端口都是随机的，就算碰巧“化身”了连接，系统内核协议栈的实现也能通过新连接的初始SYN序列号要大于TIME_WAIT连接下最后的序号大以及开启tcp_timestamps时，新连接的时间戳更大来避免。但服务端必须绑定在众所周知的端口上，因此，它首先断开连接并重启时，必须也要重用上次连接的端口，就会导致该问题。

**1，正确的处理方法是设置socket的SO_REUSEADDR选项**

1.1，允许服务端程序可以在极短的时间内，复用同相同的TIME_WAIT端口启动；

1.2，如果本机服务器有多个地址IP，它可以在不同的地址下使用相同的端口提供服务。根据客户端connect的不同IP，请求会被路由到不同的程序上，因为一个TCP连接，一定是使用一个唯一的四元组来标识；

1.3，因为四元组唯一标识一个有效连接，只要客户端程序的端口和上一个连接不同，服务端重用端口不会有任何问题。极小的概率下，碰巧客户端端口又相同，还有系统内核的SYN优化和tcp_timestamp来区分；

1.4，在bind是讲套接字和IP+端口的映射关系告诉系统内核，因此，需要在bind前设置SO_REUSEADDR才能将重用信息告诉内核，使其生效。

**2，区分tcp_tw_reuse和SO_REUSEADDR**

2.1，tcp_tw_reuse是内核态选项，SO_REUSEADDR是用户态选项；

2.2，tcp_tw_reuse常用于连接发起方主动关闭，通常为了防止客户端机器由于程序的某种bug导致的无效TIME_WAIT连接过多，导致系统的端口资源不足。而SO_REUSEADDR是服务端为了绑定在一个众所周知的端口上，必须在自己主动断开重启后，可以重用TIME_WAIT下的端口。当然，如果该端口在其他状态，同样是不能使用的。

**四，理解TCP的流**

TCP上传递的数据就像水管里的水一样，以流动的方式从发送到传送到接收端。

**1，网络字节序**

在网络字节流上，因为普通字符都是单个的字节，接收端可以拿过来直接解码。但对于数字，比如，int，float等包含多个字节的数据时，因为不同数据的存储尚未统一，存在大端字节序和小端字节序两种存储方式，解码程序需要判断数字高位到底是在字节流的前面还是后面，所以为了保证网络上传递的数据字节序相同，统一规定使用大端字节序，更符合人类的既定思维。与此同时，POSIX标准提出了统一的转换函数，对开发者屏蔽主机系统的存储顺序，帮助我们在主机字节序和网络字节序间灵活转换。

**2，网络报文解析**

虽然发送数据的顺序使TCP严格保证的，但只是一连串的字节流，并没有严格的分界，需要应用层想办法来处理数据间的分隔，确定每一条报文的边界。常见有2种方法：

2.1，发送端首先发送特定格式的消息头，接着在消息头中通知给接收端报文的长度，常见的RPC协议；

![img](.\Image\网络编程实战-4.jpg)

2.2，通过特殊字符来划分，比如，HTTP协议。

![img](.\Image\网络编程实战-5.jpg)

**五，TCP并非总是“可靠”**

TCP的可靠体现再TCP协议层是可靠的，而非应用层。如何理解这句话？

当调用write或read函数时，数据只是被发送到本机的发送缓冲区或是从接收缓冲区提取，而数据什么时候发送，是否已发送，是否需要重传，是否已经被ack确认，全部都被系统内核屏蔽在tcp的协议层实现里，应用层无法感知。比如，当tcp协议层返回ack确认后，接收端并没有办法保证ack过的数据，可以正常提交给应用程序处理。即，TCP协议的实现并没有提供给上层应用更多的异常处理细节。

因此，TCP连接建立后，应用程序只能通过以read为核心的读操作和以wirte为核心的写操作，来感知可能的异常。有如下2大类，若干小类的情况：

**1，对端无FIN包：**对端没有调用close或shutdown，应用程序可以通过如下场景感知：

**1.1，网络中断导致无FIN包**

**1），**网络中的其他设备，比如路由器，发出的一条ICMP报文不可达目的网络或主机时，**返回的“unreacheable”错误可以从read或write感知**；

**2），**如果程序**阻塞在read上**，程序无法从阻塞中恢复**，除非设置超时时间**；

**3），**如果程序**先write了一段数据流，接着阻塞在read上**，Linux的协议栈会重传12次，约9分钟后，阻塞的read返回timeout。如果程序再次重试write，会立即失败并触发SIGPIPE信号。

**1.2，程序崩溃导致无FIN包（区别于系统调用杀死进程）**

1），无任何的不可达错误；

2），同1.1.2）；

3），同1.1.3）；

4），当对端程序崩溃后重启，重用TIME_WAIT端口后，重传的TCP分组到达对端的一个不存在的连接上，**对端系统会返回RST重置分节**。阻塞read调用会立即返回“连接重置”错误，表示连接重置；write操作也会失败，并触发SIGPIPE信号。

**2，对端有FIN包：**对端正常调用close或shutdown或系统调用杀死进程时，系统内核代为清理，应用程序无法区分，但可以通过如下场景感知：

2.1，阻塞read操作正常接收缓存中数据，直到**返回值正常返回0**（EOF）；

2.2，在正常read到EOF前，发送write，接着再read，**根据不同的内核实现**，Linux 4.4内核，应用程序正常返回0；Mac OS 10.13.6内核，返回RST异常；

2.3，在正常read到EOF前，连续发送write，**根据不同的内核实现，**Linux 4.4内核，应用程序返回RST异常；Mac OS 10.13.6内核，触发SIGPIPE信号。

2.4，**注意，在TCP网络编程的学习时，一定要区分应用层接收发送逻辑和内核协议层的接收发送逻辑，也就是区分2.2和2.3。**比如，当对端协议层发送EOF，本机协议层正常接收回应ACK，但本机应用层可能并没有马上read，就不会感应到对端已退出。相反，如果本机应用层，仍在不停write，就只能通过write返回RST或触发信号中断来感知错误。

![img](.\Image\网络编程实战-6.jpg)

**六，检查数据有效性**

1，随时判断read和write返回值，并捕获SIGPIPE信号；

2，socket操作的超时机制；

3，随时防止缓冲区溢出；

4，在接收数据时，随时对变长报文头中的长度进行验证；

5，考虑使用一个全局循环缓冲区buffer对象作为应用层的读写缓冲。

**七，参考文章**

1，极客时间：网络编程实战 —— 盛延敏：（提高篇13-18讲）

## 三：性能篇_上

**一，IO 多路复用**

假设，**设计一个单线程程序**，需要从标准输入接收用户输入，并通过socket发送出去，同时，也能通过socket接收对方发送的数据流？

我们可以使用fgets等待标准输入，但就没办法从 socket 读取数据；也可以使用 read 方法等待对端数据流，但也没办法在标准输入有数据的情况下，读取数据并发送给对端。

IO 多路复用就是要解决这个场景，它将标准输入，socket 都看做是 IO 的一路，在任何一路的 IO 有事件发生的情况下，通知应用程序去处理相应的 IO 事件。即，仿佛同一时刻可以处理多个 IO 事件。

常见的 IO 事件类型很多，比如，标准输入文件描述符可以读，监听 socket 上有新连接建立，通信 socket 上有缓冲区可以写，IO 事件等待超时等。

**二，select**

select 是一种常见的 IO 多路复用技术，使用它，通知系统内核挂起进程，当一个或多个 IO 事件发生后，控制权返回给应用程序，由应用程序进行 IO 事件的处理。

**1，使用方法**

**1.1，maxfd **表示等待事件的 fd 基数，值为最大的 fd + 1，比如，有 3 个 sokcet 的 fd 分别是 {0, 1, 4}，则 maxfd = 4 + 1 = 5，原因稍后解释；

**1.2，readset** 表示读描述符集合，**writeset **表示写描述符集合，**exceptset** 表示异常描述符集合。分别可以使用 FD_ZERO，FD_SET，FD_CLR 和 FD_ISSET 来设置它们。其中，三个描述符集合可以任意设置为 NULL，表示不需要内核进行相关检测；

**1.3，timeval **结构体时间，表示等待超时时间。如果为 NULL，表示等待无限时间直到有 IO 事件发生；如果都设置为 0，表示根本不等待，立即返回；如果为正常值，则表示等待固定时间后返回。

**2，内部机制**

**2.1，select在内部维护一个描述符数组a，数组的长度为maxfd。**其中，a的下标表示待检测的fd描述符，a中的值用0和1表示该fd是否有事件发生。比如，maxfd=4，a[0]=1，a[1]=0，a[2]=1，a[3]=0表示，fd=0和fd=2的socket上有事件发生，fd=1和fd=3的socket上无事件发生。如果刷过leetcode，会很容易理解。

**2.2，对 select 内部数组 a 的操作含义**

（1），FD_ZERO 将描述符集合 a 置零；

（2），用户可以使用 FD_SET 将 a[fd] 置 1，表示 fd 等待被系统内核检测；

（3），用户可以使用 FD_CLR 将 a[fd] 置 0，表示 fd 不需要被系统内核检测；

**即，用户使用 FD_SET 和 FD_CLR 设置 a 数组，以 0 和 1 告诉系统内核，该 fd 是否需要被检测。**

（4），在 select 有事件返回时，如果待检测的 fd 上真的有事件发生，系统内核保留 a[fd] = 1 的值，若没有事件发生，系统内核将其重置为0；

**即，内核在 select 返回时，重置 a 数组，以 0 和 1 告诉用户，该 fd 是否真的有事件发生。**

（5），在 select 有事件返回后，用户可以使用 FD_ISSET，以 a[fd] 是否为 1 来判断待检测的 fd 上是否真的有事件发生。

**即，用户在完成一轮 select 后，需要再一次重置描述符集合**，将我们的待检测目标正确的告诉内核。

**2.4，a 数组只有 2 个状态，只需要 1 位就能表示，因此，a 数组可以按位来表示。**即一个 int 型的数组，如果有 n 个元素，那么，它可以表示成，长度为 n * 4 * 8 的描述符集合数组 a。

**3，就绪条件**

**3.1，内核通知 fd 可读，读操作不阻塞，但有多种情况：**接收缓冲区有数据可读，直接读到数据；对端 close，发送 FIN，读操作返回 0；监听 socket 上有新的连接，返回通信 socket；fd 上有异常或错误，读操作返回 -1。

**3.2，内核通知 fd 可写，写操作不阻塞，但有多种情况：**发送缓冲区有空余，写入应用数据；fd 的写半边已关闭，继续写产生 SIGPIPE 信号；fd 上有异常或错误，写操作返回 -1。

**三，poll**

select使用可读，可写和异常三个不同的描述符集合来描述IO事件，但它有一个确定，它支持的文件描述符是有限制的，Linux默认的最大值是1024。使用poll，因为它和内核交互的数据结构发生了变化，因此，也就能突破描述符的个数限制。

**1，poll函数介绍**

**1.1，pollfd数组：**pollfd结构由文件描述符fd+待检测的事件events+内核返回的事件revents构成。其中，fd如果为负数，内核会忽略该fd的检测；events由POLLIN或POLLOUT等选项，使用二进制掩码位操作来完成。和系统内核每次会修改传入的待检测描述符集合不同，poll因为有revents，所以就不需要在每次检测完都重置描述符为初始值。**具体来说，**

event事件主要有2大类：

**1），可读事件：**有4种可读事件，一般使用POLLIN即可。它与select的readset基本一致，都是系统内核通知应用程序有数据可以读，read不会被阻塞；

**2），可写事件：**有3种可写事件，一般使用POLLOUT即可。它与select的writeset基本一致，都是系统内核通知应用程序有数据可以写，write不会被阻塞。

revent事件主要有3大类：

**1），可读事件/可写事件：**与上文描述一样，它们可以在events中向系统内核提交检测请求，内核也可以通过revents来返回；

**2），错误事件：**不能通过events向内核提交请求，应用程序只能通过revents加以检测（POLLERR：错误发送；POLLHUP：fd挂起；POLLNVAL：事件无效）。

**1.2，nfds：**pollfd数组的大小。表示应用程序通过poll向内核申请的fd检测个数，这说明，用户可以自己指定待检测的fd数量，而不受内核的限定；

**1.3，timeout：**超时时间。<0：有事件前永远等待；=0：不阻塞，立即返回；>0：等待指定毫秒数后返回；

**1.4，返回值：**-1：有错误发生；0：没有事件发生；>0：发生检测到发生事件的fd个数。

**2，使用方法**

流程上，和select类似，略。

**四，epoll**

epoll 是先进Linux网络编程领域被使用最多的 IO 多路复用技术，在待检测描述符极多的情况下，相比select和poll，性能极高。如下图所示：

![img](.\Image\网络编程实战-7.jpg)

**1，使用方法**

**1.1，epoll_create：**创建一个epoll实例，因为Linux 2.6.8可以根据监控的描述符数目大小而动态分配内核数据结构，size参数只需要填一个大于0的数即可；

**1.2，epoll_ctl：**增加或删除待监控的描述符。其中，op参数表示对epoll实例的操作，有3个选项：EPOLL_CTL_ADD：增加；EPOLL_CTL_DEL：删除；EPOLL_CTL_MOD：修改。epoll_event：包含待监控的EPOLL事件events，类型与poll事件类似（额外包含了边缘触发事件选项），也支持按位掩码。除此之外，epoll_event参数还可以携带epoll_data数据，通常需要填入待检测的fd，供内核分发IO事件后使用；

**1.3，epoll_wait：**类似调用select或poll函数，调用者进场被挂起，等待内核IO事件的分发，其中，events返回给用户空间需要处理的IO事件，大小受maxevents限制，由函数返回值决定。

**2，性能优势**

2.1，==**epoll监控注册的多个描述符，和select/poll返回fd的数目，而让应用程序自己去遍历检测不同，epoll直接告诉上层，哪些描述符上有事件发生，直接返回来分发IO事件，针对性更强；**==

2.2，==**select和poll都是使用水平触发，如果上层应用数据没读完，会反复通知有事件发生。epoll在此基础上，还提供了边缘触发的能力，即当fd上有事件发生时，只通知一次，让上层应用自己把数据取完。**==

**3，历史溯源**

Windows在1994年引入真正的异步IO模型，FreeBSD在2000年引入Kqueue作为IO事件分发框架，epoll则在2002年才被引入至Linux，至于epoll为什么采用与select和poll类似的数组结构，而不用Kqueue的队列结构呢，因为大神Linus在一开始就把queue的结构排除在外，建议将数组的结构作为基本准则。

**五，IO 模型介绍**

在此讲以前，几乎所有的讨论都建立在阻塞 IO 的前提下，从这讲开始，我们将进入非阻塞 IO 的世界。结论：IO 多路复用配合非阻塞 IO，是高性能网络编程里的常见技术。

**1，阻塞 IO vs. 非阻塞 IO：**对 socket 设置不同的选项，就可以设置和切换

1.1，应用程序使用阻塞 IO 处理某个操作时，程序会被挂起，等待内核完成。此时，内核将 CPU 时间切换给其他进程，直到内核准备好，将数据从内核缓冲区拷贝到应用程序的 socket 缓冲区；

1.2，应用程序使用非阻塞 IO 处理某个操作时，内核立即返回，如果没有准备好，内核不会让渡 CPU 给其他进程，自己仍然占用 CPU 的执行时间片。

**2，非阻塞 IO + 轮询 vs. 非阻塞 IO + IO 多路复用**

2.1，应用程序可以不让渡 CPU，并使用轮询检测的方法，询问内核是否准备好，但空转 CPU 的同时，只能监控一个 IO 操作，性价比太低；

2.2，应用程序使用 IO 多路复用技术，让内核分发不同的 IO 事件，告知应用程序数据是否准备好。在 timeout 前，应用程序挂起，让渡 CPU，但这是由 IO 多路复用触发的，并且可以同时监控多个不同的 IO 操作；

2.3，**衍生问题：**IO 多路复用已经返回了 fd 有事件发生，可读或者可写，直接读写 fd 并不会发生阻塞，为什么还要用非阻塞 IO 呢？（参考：http://zhihu.com/question/37271342）

1），当 select / poll / epoll 报告数据可读后，由于检查 checksum 发生错误，可能会被内核丢弃，如果调用 read 可能会导致阻塞；

2），当 select / poll / epoll 报告数据可读后，必须使用水平触发，且每次只能调用一次 read，否则，极有可能在第 2 次调用的时候，数据已经被读完，导致阻塞；

3），当多个线程监控同一个 fd 时，select / pol l/ epoll 会在所有的线程中返回，但只有一个线程能够读到数据，其他线程就会被意外阻塞；

4），若是监听套接字被设置为阻塞，当有新的连接进来后，多路复用函数返回，但在调用 accept 之前，客户端断开，发送 RST 分节，内核将此连接从已完成队列中删除，服务端再调用 accept 则会被阻塞，该线程再也无法分发其他的 IO 事件。

**3，同步 IO vs. 异步 IO：**需要由操作系统的具体实现来支持

3.1，针对 read 和 write 的函数调用而言，不管是阻塞 IO，非阻塞 IO + 轮询，非阻塞 IO + IO 多路复用，最后一次调用 read 或者 write 操作时，均为同步 IO 操作，它们都是等待内核完成数据在用户空间和内核空间的拷贝后，函数才真正返回。这里只是，在内核尚未准备好时，应用程序是否会挂起，让渡 CPU，或者，是自己轮询还是内核通知的区别而已。

3.2，针对 aio_read 和 aio_wirte 的函数调用而言，调用后函数立即真正返回。数据在不同空间之间的拷贝，是异步完成的，在拷贝的途中，应用程序可以去干其他的事情。待到拷贝完成后，内核通过回调函数，告诉应用程序可以获取数据，开始数据部分的处理。

**六，详解非阻塞 IO 的使用场景**

**1，读操作：**只要接收缓冲区有数据，不管阻塞还是非阻塞 IO，read 都会立即返回。区别在于，当缓冲区内无数据可读，阻塞 IO 挂起等待，而非阻塞 IO 立即返回，并伴随 EWOULDBLOCK 或 EAGAIN 的错误。错误提示表明，数据尚未准备好，你可以自己阻塞或者再次轮询后检查；

**2，写操作：**和读操作不同，阻塞 IO 下，write 必须等待要求的数据全部拷贝至发送缓冲区后才返回，即返回的字节数和输入参数中要求的字节数相同。而非阻塞 IO，它会告诉内核尽最大可能拷贝数据至发送缓冲区后，当发送缓冲区有空闲时，就通过返回值，返回拷贝成功的数据量；当发送缓冲区满，一个字节都没拷贝，返回 -1 并同样伴随 EWOULDBLOCK 或 EAGAIN 的错误。

**七，参考文章**

1，极客时间：网络编程实战 —— 盛延敏：（性能篇20-23讲）

## 三：性能篇_下

**一，C10K问题**

随着互联网的蓬勃发展，尤其是 2000 年前后，互联网人数井喷，C10K 问题也就应运而生，即，如何在一台物理机上同时服务 10000 个用户（Concurrent 10 * 1000）。

**二，操作系统层面**

C10K 问题要求在一台主机上至少同时支持 1 万个连接，需要文件句柄，系统内存，网络带宽达到什么程度呢？

**1，文件句柄：**每个客户连接，代表一个文件描述符，一旦文件描述符不够用时，新的连接就会被放弃，报错 “Socket/File: Can’t open too many files”。虽然在默认情况下，单进程打开的文件句柄数是有限制的（ulimit -n -> 1024），但可以通过修改 /etc/sysctl.conf 文件的 file-max，ip_conntrack_max 等参数为 10000，使单进程能打开的文件描述符超过 10000；

**2，系统内存：**每个 TCP 连接，除了占有一个文件描述符外，还需要占用一定的发送缓冲区和接收缓冲区的内存，可以使用 /proc/sys/net/ipv4/tcp_wmem 和 tcp_rmem 来查看（最小分配值 ( 4k / 4k )，默认分配值 ( 16k / 87k ) 和最大分配值 ( 4m / 6m ) ），按默认分配值计算，合并上应用层也需要一个收发数据的业务层 buffer（比如：20k ），则 C10K 要求单服务器至少需要 (16 + 87 + 20 ) k * 10000 = 1.23G 的内存，这在现代计算机上也不是大问题；

**3，网络带宽：**假设 1 万个连接，每个连接上每秒传输 10k 的数据，带宽需要 10k * 8bit / k* 1000 = 800 Mbps，这在现在标配的万兆网卡上，也不是问题；

**4，系统结论：**虽然在 2000 年左右，C10K 问题在系统资源上，可能也有瓶颈，但在现在而言是可以解决的，因此，C10K 的瓶颈自然就落到了程序设计层面。

**三，程序设计层面**

==网路编程中，涉及到频繁的用户态和内核态的数据拷贝==，一旦设计不好，在高并发下，性能很容易出现指数级下降，因此，在程序设计上，需要考虑2方面的问题：

1，应用程序如何和操作系统配合，感知 IO 事件发生，调度处理上万个 socket 上的 IO 操作？这在前面已有详细描述。即，阻塞 IO，非阻塞 IO，IO 多路复用讨论的 IO 模型就是解决这方面的问题；

2，应用程序如何分配进程和线程资源，使其可以服务上万个连接？下面详细讨论。

**四，C10K 解决方案**

任何一个网络程序，均包含 read：从套接字读，decode：从网络流解码，compute：业务逻辑，encode：编码成网络流，send：通过套接字发送，共计 5 步，有以下 5 种编程模型来尝试解决 C10K 问题。

**1，阻塞 IO + 进程**

每个连接 fork 一个子进程处理，由子进程处理该连接上的所有 IO，所有连接相互隔离。实现简单，效率不高，资源利用率高。

**1.1，fork 简介：**fork 函数在实现的时候，会把当前父线程的所有资源都拷贝一份，包括执行代码，地址空间，打开的文件描述符和程序计数器等。注意，Linux 是懒拷贝，直到真正发生写操作时，才会真正拷贝资源。fork 返回后，父线程返回子线程 pid，子线程返回 0，出错返回 -1。此时，根据不同的返回值，执行不同的分支逻辑，即创建子进程成功。

**1.2，子进程退出：**进程号为 1 的 init 进程是所有用户态进程的祖先进程。当一个子进程退出时，因为系统内核还保留了该进程的若干信息，如果父进程忽略了对子进程的回收，子进程就会被挂到 init 进程下，变成僵尸进程，无谓消耗系统资源。

**1.3，关闭子进程：**使用 signal 捕捉子进程退出或中断时，内核向父进程发出的 SIGCHILD 信号，并在信号处理函数中使用 wait 或 waitpid 关闭子进程。两个函数都有 2 个返回值：1 个是函数返回值表示已终止的子进程 ID，另一个是入参 statloc 指针返回子进程的终止状态，比如，正常终止，被信号杀死或者作业控制停止等。wait 等待第 1 个进程终止，否则阻塞；waitpid 提供 pid 参数，可以指定等待终止的进程。

**1.4，注意事项：**在子进程分支中 close 监听 fd，在主进程分支中 close 连接 fd，否则，因为 fork 进程后，fd 的引用计数增加，如果此处忘记 fork，会导致 fd 的泄漏。

![img](.\Image\网络编程实战-8.jpg)

**2，阻塞 IO + 线程**

每个连接 pthread_create 一个单独的线程，由线程处理该连接上的所有 IO，效果与方法 1 类似。在此基础上，可以通过预先创建线程池的方式，在多个连接中复用线程池提升效率。

**2.1，线程简介：**pthread_create 在使用系统调用，进入内核态创建真正的线程时，子线程会持有进程内所有的资源的指针，即共享资源，虽然在内核的视角内都是task，但是要注意和进程区别。

**2.2，POSIX 线程模型：**现代 UNIX 系统提供的处理线程的标准接口，pthread 是对它的实现之一。通过它，可以依照“阻塞 IO + 进程”的模式处理，完成“阻塞IO + 线程”的实现。

**2.3，线程池方案：**频繁创建和关闭线程，开销较大，可以使用线程池优化。常用的线程池实现使用“生产者-消费者模式”，使用一个 fd 阻塞队列，使用管程中“等待-通知”的机制，主线程 accept 成功后往队列里新增 fd，线程池竞争队列里的 fd 进行服务。因为线程池大小固定，在这种方案下，更适合短连接场景，同时，肯定也会有连接得到不及时服务的情形，需要注意。

![img](.\Image\网络编程实战-9.jpg)

**3，非阻塞 IO + IO 多路复用 + 单线程：**向系统内核注册待监控的连接，由操作系统来分发 IO 事件，告知应用程序哪些可读，哪些可写，应用最后再进行遍历处理。

**3.1，模型简介：**事件驱动模型，reactor 模型或者是 event loop 模型，有2个核心点：（1）它存在一个无限循环的时间分发线程，或者叫 reactor 线程 / event loop 线程。该线程背后，就是 poll 或者 epoll 等多路复用技术的使用；（2）所有的 IO 操作都可以抽象成事件，每个事件必须有回调函数处理。连接事件，读事件或者写事件都需要被检测，并使用回调函数处理。

**3.2，模型缺点：**本质上，该模式是符合大规模生产需求的，reactor dispatch 是一个单独的分发线程，应用程序在线程内，依次处理不同的连接事件和 IO 读写事件。但这个模式有一个潜在的缺点，compute 的业务操作通常是非常耗时的，分发线程在处理业务逻辑的时候，它是没有办法处理其他连接或 IO 读写事件的，因此，它会成为整个模式的性能瓶颈。

![img](.\Image\网络编程实战-10.jpg)

**4，非阻塞 IO + IO 多路复用 + 多线程：**使用多线程的技术，让操作系统在不同的线程里，分发不同的 IO 事件，提高效率。即，主 - 从 reactor 模式。大多数高性能网络编程框架都是以 epoll 技术为核心的主 - 从 reactor 模式，主模式监听连接事件，从模式处理其他 IO 事件。

**4.1，模式简介：**因为单线程分发的瓶颈问题，reactor dispatch 线程将耗时的 decode，compute 和 encode 组织成一个一个的小任务，分发至另一个工作线程池中处理，自己只负责 read 和 send 的相关工作，实现逻辑解耦，提高效率。

**4.2，模式缺点：**该模式在 reactor dispatch 线程上，同时分发 acceptor，IO 读写事件，在瞬时发起连接的请求非常多的情况下，reactor dispatch 由于忙着处理 IO 读写事件，使得客户端的连接成功率偏低。

![img](.\Image\网络编程实战-11.jpg)

**4.2，模式进阶：**主 - 从 reactor 模式，主 reactor dispatcher 只负责分发 acceptor 的连接事件，而将已连接的 IO 读写事件，交给从 sub-reactor dispatcher 分发，其中从 reactor 的数量，可以按需配置。注意，同一个 fd 连接上的 IO 读写事件，可以 hash 至同一个 sub-reactor 线程上，固定由该线程分发其 IO 读写事件，减少并发处理的锁开销。最后，配合工作线程池处理 decode，compute 和e ncode 工作，就构成的实战中，最常见的网络编程范式，比如，Netty。另外，还有一个需要注意的点是缓冲区对象 buffer 的设计，在进行网络编程的框架设计时，我们希望框架本身可以对上层应用屏蔽 socket 的读写操作，转而提供对缓冲区对象 buffer 的读写操作，使应用程序的处理更简单。

**4.3，epoll 的使用：**在高性能服务器编程中，相比 select 和 poll，**epoll 的高性能主要体现在3个方面**：==其一，每次在使用 poll 和 select 前，都需要重新准备一个待监控事件集合交给系统内核重新构建数据结构完成注册，而 epoll 是维护了一个全局事件集合，通过 epoll 句柄来管理；其二，每次在使用 poll 和 select 后，返回的都是就绪的事件个数，需要应用程序去遍历集合逐个判断，而 epoll 直接就将就绪事件集合返回，避免无效遍历；其三，poll 和 select 默认使用水平触发，当应用程序处理未完成时，需要内核不断通知应用，消耗性能，而 epoll 提供边缘触发，干脆只通知应用程序一次，你自己想办法处理，提高性能。==

![img](.\Image\网络编程实战-12.jpg)

**5，异步 IO + 多线程：**前面 4 种在 read 或 write 时，都是同步调用，需要等待系统内核完成 socket 缓冲区至应用程序缓冲区的拷贝。但异步 IO 调用结束后， aio_reade 或 aio_write 立即返回，当系统内核完成缓冲区靠背后，产生一个信号或执行回调函数来完成 IO 处理。对异步 IO 与阻塞/非阻塞/多路复用 IO 的对比，详见“性能篇_上篇”。虽然异步 IO 是最终大杀器，但和 windows-IOCP 不同，Linux 下的 aio 操作，并非真正的操作系统级别的支持，只是 libc 库在用户空间借助 pthread 的方式实现的针对磁盘类读写的 IO 操作，不支持 Socket IO 操作。

**五，课后实践**

**一定！一定！一定！**

要自己动手，实现一个高性能（10k-20k并发量的HTTP echo服务器）。

**六，参考文章**

1，极客时间：网络编程实战 —— 盛延敏：（性能篇24-30讲：完）